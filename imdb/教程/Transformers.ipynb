{"cells":[{"cell_type":"markdown","metadata":{"id":"9dF1QhmDp_tN"},"source":["# task6：使用Transformer进行情感分析"]},{"cell_type":"markdown","metadata":{"id":"mzpdgYgdp_tP"},"source":["在本notebook中，我们将使用在 [Attention is all you need](https://arxiv.org/abs/1706.03762) 论文中首次引入的Transformer模型。 具体来说，我们将使用 [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805) 论文中的 BERT模型。\n","\n","Transformer 模型比本教程中涵盖的其他模型都要大得多。 因此，我们将使用 [transformers library](https://github.com/huggingface/transformers) 来获取预训练的Transformer并将它们用作我们的embedding层。 我们将固定（而不训练）transformer，只训练从transformer产生的表示中学习的模型的其余部分。 在这种情况下，我们将使用双向GRU继续提取从Bert embedding后的特征。最后在fc层上输出最终的结果。"]},{"cell_type":"code","execution_count":29,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2147,"status":"ok","timestamp":1676996241183,"user":{"displayName":"单元康","userId":"15355905598088806336"},"user_tz":0},"id":"sfbxDb-fWeD_","outputId":"5f66654a-4373-4a95-9960-e98adf315820"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"tQbWMgCmp_tQ"},"source":["## 6.1 数据准备\n","\n","首先，像往常一样，我们导入库，然后设置随机种子"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1575,"status":"ok","timestamp":1675607519271,"user":{"displayName":"单元康","userId":"15355905598088806336"},"user_tz":-480},"id":"NOOaEsDvVRjl","outputId":"b5677f97-246f-4a0e-bc5d-de5fb6217662"},"outputs":[{"output_type":"stream","name":"stdout","text":["Sun Feb  5 14:31:58 2023       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   69C    P0    30W /  70W |      0MiB / 15360MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}],"source":["!nvidia-smi"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fq9trNGUT8Vu"},"outputs":[],"source":["# pip install torch==1.8.0+cu102 torchvision==0.9.0+cu102 torchaudio==0.8.0 -f https://download.pytorch.org/whl/torch_stable.html"]},{"cell_type":"code","execution_count":50,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3475,"status":"ok","timestamp":1676996970157,"user":{"displayName":"单元康","userId":"15355905598088806336"},"user_tz":0},"id":"FbhyUK5dKb4y","outputId":"bcc82b11-4689-4989-fcae-436c4b13b29f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Looking in links: https://download.pytorch.org/whl/torch_stable.html\n","Requirement already satisfied: torch==1.8.0+cu111 in /usr/local/lib/python3.8/dist-packages (1.8.0+cu111)\n","Requirement already satisfied: torchtext==0.9.0 in /usr/local/lib/python3.8/dist-packages (0.9.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torch==1.8.0+cu111) (1.21.6)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch==1.8.0+cu111) (4.5.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchtext==0.9.0) (2.25.1)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from torchtext==0.9.0) (4.64.1)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torchtext==0.9.0) (2022.12.7)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torchtext==0.9.0) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torchtext==0.9.0) (2.10)\n","Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->torchtext==0.9.0) (4.0.0)\n"]}],"source":["pip install torch==1.8.0+cu111 torchtext==0.9.0 -f https://download.pytorch.org/whl/torch_stable.html"]},{"cell_type":"code","source":["pip install torch==1.8.1 torchtext==0.9.0"],"metadata":{"id":"sDTiHK8Expa8"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":177,"status":"ok","timestamp":1676996180260,"user":{"displayName":"单元康","userId":"15355905598088806336"},"user_tz":0},"id":"dt1GSi-3fw8w","outputId":"190a0feb-4cdb-43de-8c72-73a02f32a692"},"outputs":[{"output_type":"stream","name":"stdout","text":["nvcc: NVIDIA (R) Cuda compiler driver\n","Copyright (c) 2005-2022 NVIDIA Corporation\n","Built on Tue_Mar__8_18:18:20_PST_2022\n","Cuda compilation tools, release 11.6, V11.6.124\n","Build cuda_11.6.r11.6/compiler.31057947_0\n"]}],"source":["!nvcc --version"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":148,"status":"ok","timestamp":1676996181777,"user":{"displayName":"单元康","userId":"15355905598088806336"},"user_tz":0},"id":"Pqhhd3P1eLRs","outputId":"65af018b-95e4-4ef7-c571-80a1ddbdb064"},"outputs":[{"output_type":"stream","name":"stdout","text":["No LSB modules are available.\n","Distributor ID:\tUbuntu\n","Description:\tUbuntu 20.04.5 LTS\n","Release:\t20.04\n","Codename:\tfocal\n"]}],"source":["!lsb_release -a"]},{"cell_type":"code","execution_count":31,"metadata":{"id":"oB94YMUBaVls","executionInfo":{"status":"ok","timestamp":1676996256607,"user_tz":0,"elapsed":171,"user":{"displayName":"单元康","userId":"15355905598088806336"}}},"outputs":[],"source":["import os\n","os.chdir('/content/drive/My Drive')"]},{"cell_type":"code","execution_count":32,"metadata":{"id":"4GelcjPnp_tQ","executionInfo":{"status":"ok","timestamp":1676996258379,"user_tz":0,"elapsed":2,"user":{"displayName":"单元康","userId":"15355905598088806336"}}},"outputs":[],"source":["import torch\n","\n","import random\n","import numpy as np\n","\n","SEED = 1234\n","\n","random.seed(SEED)\n","np.random.seed(SEED)\n","torch.manual_seed(SEED)\n","torch.backends.cudnn.deterministic = True"]},{"cell_type":"code","execution_count":33,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":167,"status":"ok","timestamp":1676996258825,"user":{"displayName":"单元康","userId":"15355905598088806336"},"user_tz":0},"id":"zCBmqrW4XwQy","outputId":"b1d0819a-635f-4b86-e21a-e6200c687649"},"outputs":[{"output_type":"stream","name":"stdout","text":["True\n","cuda\n"]}],"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(torch.cuda.is_available())\n","print(device)"]},{"cell_type":"markdown","metadata":{"id":"QAPlbbaTp_tR"},"source":["Transformer 已经用特定的词汇进行了训练，这意味着我们需要使用完全相同的词汇进行训练，并以与 Transformer 最初训练时相同的方式标记我们的数据。\n","\n","幸运的是，transformers 库为每个提供的transformer 模型都有分词器。 在这种情况下，我们使用忽略大小写的 BERT 模型（即每个单词都会小写）。 我们通过加载预训练的“bert-base-uncased”标记器来实现这一点。"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10198,"status":"ok","timestamp":1676995974028,"user":{"displayName":"单元康","userId":"15355905598088806336"},"user_tz":0},"id":"QxkrPf552L-8","outputId":"5e5f17cf-8040-4d46-857f-704aa6e25945"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.26.1-py3-none-any.whl (6.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m47.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.9.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n","  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m107.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (23.0)\n","Collecting huggingface-hub<1.0,>=0.11.0\n","  Downloading huggingface_hub-0.12.1-py3-none-any.whl (190 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.3/190.3 KB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n","Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n","Installing collected packages: tokenizers, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.12.1 tokenizers-0.13.2 transformers-4.26.1\n"]}],"source":["!pip install transformers"]},{"cell_type":"code","execution_count":34,"metadata":{"executionInfo":{"elapsed":167,"status":"ok","timestamp":1676996264715,"user":{"displayName":"单元康","userId":"15355905598088806336"},"user_tz":0},"id":"tjcr5xifp_tR"},"outputs":[],"source":["from transformers import BertTokenizer\n","\n","\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"]},{"cell_type":"markdown","metadata":{"id":"ZjAUeoVwp_tS"},"source":["`tokenizer` 有一个 `vocab` 属性，它包含我们将使用的实际词汇。 我们可以通过检查其长度来检查其中有多少单词。"]},{"cell_type":"code","execution_count":35,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1,"status":"ok","timestamp":1676996267375,"user":{"displayName":"单元康","userId":"15355905598088806336"},"user_tz":0},"id":"cEqBo1avp_tS","outputId":"fac65a59-2b16-4e8f-91e8-003ded8a67f3"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["30522"]},"metadata":{},"execution_count":35}],"source":["len(tokenizer.vocab)"]},{"cell_type":"markdown","metadata":{"id":"N_wwqMT0p_tS"},"source":["使用`tokenizer.tokenize`方法对字符串进行分词，并统一大小写。"]},{"cell_type":"code","execution_count":36,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1,"status":"ok","timestamp":1676996268939,"user":{"displayName":"单元康","userId":"15355905598088806336"},"user_tz":0},"id":"9UjuAd3Lp_tS","outputId":"fd8d9cfb-6602-4be3-9357-05a29b6364c7"},"outputs":[{"output_type":"stream","name":"stdout","text":["['hello', 'world', 'how', 'are', 'you', '?']\n"]}],"source":["tokens = tokenizer.tokenize('Hello WORLD how ARE yoU?')\n","\n","print(tokens)"]},{"cell_type":"markdown","metadata":{"id":"kJ53CkKlp_tS"},"source":["我们可以使用我们的词汇表使用 `tokenizer.convert_tokens_to_ids` 来数字化标记。下面的tokens是我们之前上面进行了分词和统一大小写之后的list。"]},{"cell_type":"code","execution_count":37,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1676996269407,"user":{"displayName":"单元康","userId":"15355905598088806336"},"user_tz":0},"id":"uZ3sj_bmp_tT","outputId":"8d687e2f-fd93-43d9-b729-04a345090263"},"outputs":[{"output_type":"stream","name":"stdout","text":["[7592, 2088, 2129, 2024, 2017, 1029]\n"]}],"source":["indexes = tokenizer.convert_tokens_to_ids(tokens)\n","\n","print(indexes)"]},{"cell_type":"markdown","metadata":{"id":"-xCF7w9Ep_tT"},"source":["Transformer还接受了特殊tokens的训练，以标记句子的开头和结尾， [详细信息](https://huggingface.co/transformers/model_doc/bert.html#transformers.BertModel)。 就像我们标准化padding和未知的token一样，我们也可以从`tokenizer`中获取这些。\n","\n","**注意**：`tokenizer` 确实有序列开始和序列结束属性（`bos_token` 和 `eos_token`），但我们没有对此进行设置，并且不适用于我们本次训练的transformer。"]},{"cell_type":"code","execution_count":38,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1676996271462,"user":{"displayName":"单元康","userId":"15355905598088806336"},"user_tz":0},"id":"j1uiViM1p_tT","outputId":"61ca834d-3167-47fa-c414-63589f522717"},"outputs":[{"output_type":"stream","name":"stdout","text":["[CLS] [SEP] [PAD] [UNK]\n"]}],"source":["\n","init_token = tokenizer.cls_token\n","eos_token = tokenizer.sep_token\n","pad_token = tokenizer.pad_token\n","unk_token = tokenizer.unk_token\n","\n","print(init_token, eos_token, pad_token, unk_token)"]},{"cell_type":"markdown","metadata":{"id":"oAKZ_kwIp_tT"},"source":["我们可以通过反转词汇表来获得特殊tokens的索引"]},{"cell_type":"code","execution_count":39,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1676996271462,"user":{"displayName":"单元康","userId":"15355905598088806336"},"user_tz":0},"id":"98fv4cElp_tT","outputId":"a368dec4-415a-4de3-8ff0-d07df273c7aa"},"outputs":[{"output_type":"stream","name":"stdout","text":["101 102 0 100\n"]}],"source":["init_token_idx = tokenizer.convert_tokens_to_ids(init_token)\n","eos_token_idx = tokenizer.convert_tokens_to_ids(eos_token)\n","pad_token_idx = tokenizer.convert_tokens_to_ids(pad_token)\n","unk_token_idx = tokenizer.convert_tokens_to_ids(unk_token)\n","\n","print(init_token_idx, eos_token_idx, pad_token_idx, unk_token_idx)"]},{"cell_type":"markdown","metadata":{"id":"gHGGVdnOp_tU"},"source":["或者通过tokenizer的方法直接获取它们"]},{"cell_type":"code","execution_count":40,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1676996272275,"user":{"displayName":"单元康","userId":"15355905598088806336"},"user_tz":0},"id":"ylGwMTOep_tU","outputId":"346d8e19-ed5d-4a56-e5c4-0b7ab2d362a6"},"outputs":[{"output_type":"stream","name":"stdout","text":["101 102 0 100\n"]}],"source":["init_token_idx = tokenizer.cls_token_id\n","eos_token_idx = tokenizer.sep_token_id\n","pad_token_idx = tokenizer.pad_token_id\n","unk_token_idx = tokenizer.unk_token_id\n","\n","print(init_token_idx, eos_token_idx, pad_token_idx, unk_token_idx)"]},{"cell_type":"markdown","metadata":{"id":"U3Y54JCBp_tU"},"source":["我们需要处理的另一件事是模型是在具有定义的最大长度的序列上训练的——它不知道如何处理比训练更长的序列。 我们可以通过检查我们想要使用的转换器版本的 `max_model_input_sizes` 来获得这些输入大小的最大长度。"]},{"cell_type":"code","execution_count":41,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1676996273455,"user":{"displayName":"单元康","userId":"15355905598088806336"},"user_tz":0},"id":"TAxgb9eMp_tU","outputId":"7d9b73f9-7228-4018-de00-3e68c19430be"},"outputs":[{"output_type":"stream","name":"stdout","text":["512\n"]}],"source":["max_input_length = tokenizer.max_model_input_sizes['bert-base-uncased']\n","\n","print(max_input_length)"]},{"cell_type":"markdown","metadata":{"id":"pW1eDfUwp_tU"},"source":["之前我们使用了 `spaCy` 标记器来标记我们的示例。 然而，我们现在需要定义一个函数，我们将把它传递给我们的 `TEXT` 字段，它将为我们处理所有的标记化。 它还会将令牌的数量减少到最大长度。 请注意，我们的最大长度比实际最大长度小 2。 这是因为我们需要向每个序列附加两个标记，一个在开头，一个在结尾。"]},{"cell_type":"code","execution_count":42,"metadata":{"id":"KvQWvwY1p_tV","executionInfo":{"status":"ok","timestamp":1676996274855,"user_tz":0,"elapsed":466,"user":{"displayName":"单元康","userId":"15355905598088806336"}}},"outputs":[],"source":["def tokenize_and_cut(sentence):\n","    tokens = tokenizer.tokenize(sentence) \n","    tokens = tokens[:max_input_length-2]\n","    return tokens"]},{"cell_type":"markdown","metadata":{"id":"_tEcXuCcp_tV"},"source":["现在我们开始定义我们的字段,transformer期望将batch维度放在第一维上，所以我们设置了 `batch_first = True`。 现在我们已经有了文本的词汇数据，由transformer提供，我们设置 `use_vocab = False` 来告诉 torchtext 已经不需要切分数据了。 我们将 `tokenize_and_cut` 函数作为标记器传递。 `preprocessing` 参数是一个函数，这是我们将token转换为其索引的地方。 最后，我们定义特殊的token——注意我们将它们定义为它们的索引值而不是它们的字符串值，即“100”而不是“[UNK]”这是因为序列已经被转换为索引。\n","\n","我们像以前一样定义标签字段。"]},{"cell_type":"code","execution_count":51,"metadata":{"id":"uVWkdvoxp_tV","colab":{"base_uri":"https://localhost:8080/","height":522},"executionInfo":{"status":"error","timestamp":1676997008824,"user_tz":0,"elapsed":172,"user":{"displayName":"单元康","userId":"15355905598088806336"}},"outputId":"244bf6ce-76a6-4e8d-c5d7-ce217bf97084"},"outputs":[{"output_type":"error","ename":"ImportError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m<ipython-input-51-667d380b91ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m TEXT = data.Field(batch_first = True,\n\u001b[1;32m      4\u001b[0m                   \u001b[0muse_vocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                   \u001b[0mtokenize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenize_and_cut\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torchtext/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mexperimental\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlegacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torchtext/experimental/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0m__all__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'datasets'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'transforms'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'models'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torchtext/experimental/transforms.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_torchtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRegexTokenizer\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mRegexTokenizerPybind\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_torchtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSentencePiece\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mSentencePiecePybind\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mImportError\u001b[0m: /usr/local/lib/python3.8/dist-packages/torchtext/_torchtext.so: undefined symbol: _ZNK3c104Type14isSubtypeOfExtERKSt10shared_ptrIS0_EPSo","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}],"source":["from torchtext.legacy import data\n","\n","TEXT = data.Field(batch_first = True,\n","                  use_vocab = False,\n","                  tokenize = tokenize_and_cut,\n","                  preprocessing = tokenizer.convert_tokens_to_ids,\n","                  init_token = init_token_idx,\n","                  eos_token = eos_token_idx,\n","                  pad_token = pad_token_idx,\n","                  unk_token = unk_token_idx)\n","\n","LABEL = data.LabelField(dtype = torch.float)"]},{"cell_type":"markdown","metadata":{"id":"Edv3w0NKp_tV"},"source":["加载数据，拆分成训练集和验证集"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GyTK5JTKp_tV"},"outputs":[],"source":["from torchtext.legacy import datasets\n","\n","train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)\n","\n","train_data, valid_data = train_data.split(random_state = random.seed(SEED))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1675607943193,"user":{"displayName":"单元康","userId":"15355905598088806336"},"user_tz":-480},"id":"sL2HapZQp_tW","outputId":"c8ab0946-ed25-4f4b-b787-5c06e78a7595"},"outputs":[{"output_type":"stream","name":"stdout","text":["Number of training examples: 17500\n","Number of validation examples: 7500\n","Number of testing examples: 25000\n"]}],"source":["print(f\"Number of training examples: {len(train_data)}\")\n","print(f\"Number of validation examples: {len(valid_data)}\")\n","print(f\"Number of testing examples: {len(test_data)}\")"]},{"cell_type":"markdown","metadata":{"id":"FGY5EeIEp_tW"},"source":["随便看一个例子，看下具体效果如何,输出其中一个句子的one-hot向量。"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16,"status":"ok","timestamp":1675607943192,"user":{"displayName":"单元康","userId":"15355905598088806336"},"user_tz":-480},"id":"BchDPI4Xp_tW","outputId":"7cb5d5f8-96ba-4190-b228-18dc32fb4b39"},"outputs":[{"output_type":"stream","name":"stdout","text":["{'text': [1045, 2156, 1037, 2843, 1997, 5691, 1012, 2387, 1996, 2434, 2162, 26393, 2086, 3283, 1998, 3866, 2009, 1012, 7588, 2073, 2145, 1000, 1037, 2502, 6547, 1000, 2005, 1996, 2087, 1997, 2149, 1998, 1996, 3185, 2001, 13359, 1010, 1999, 2009, 1005, 1055, 2219, 2126, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 2023, 2028, 1010, 2174, 1010, 3504, 2066, 1037, 2659, 5166, 9317, 2305, 2569, 1012, 2561, 10231, 1010, 2013, 2707, 2000, 3926, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 1996, 5436, 2003, 2061, 5410, 1010, 2017, 2180, 1005, 1056, 2903, 2009, 2127, 2017, 2156, 1996, 3185, 1006, 2029, 1045, 2052, 2025, 16755, 1999, 1996, 2034, 2173, 1007, 1012, 1045, 2064, 2025, 2391, 2041, 2028, 3364, 2008, 2941, 2106, 1037, 2204, 3105, 1999, 2023, 3185, 1012, 2021, 4931, 1010, 2007, 2008, 5896, 1045, 2052, 1005, 2310, 2042, 4527, 2065, 3087, 2071, 2079, 1037, 2204, 3105, 3772, 1012, 7167, 1997, 18856, 17322, 5019, 1012, 1039, 5856, 3504, 2066, 2009, 1005, 1055, 2579, 2041, 1997, 1996, 1005, 6564, 2544, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 8670, 2232, 1010, 2053, 1010, 1045, 1005, 1049, 2893, 1999, 1037, 2919, 6888, 2074, 3015, 2055, 2023, 1012, 2079, 2025, 3422, 2023, 3185, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 2166, 1005, 1055, 2000, 2460, 2000, 5949, 2009, 2006, 3666, 10231, 7685, 5691, 1012], 'label': 'neg'}\n"]}],"source":["print(vars(train_data.examples[6]))"]},{"cell_type":"markdown","metadata":{"id":"CLK4lIZgp_tW"},"source":["我们可以使用 `convert_ids_to_tokens` 将这些索引转换回可读的tokens。"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1675607943196,"user":{"displayName":"单元康","userId":"15355905598088806336"},"user_tz":-480},"id":"wS_hBvr-p_tW","outputId":"c7fd652c-f111-4659-9977-1b589c5c1d95"},"outputs":[{"output_type":"stream","name":"stdout","text":["['i', 'see', 'a', 'lot', 'of', 'movies', '.', 'saw', 'the', 'original', 'war', '##games', 'years', 'ago', 'and', 'loved', 'it', '.', 'computers', 'where', 'still', '\"', 'a', 'big', 'mystery', '\"', 'for', 'the', 'most', 'of', 'us', 'and', 'the', 'movie', 'was', 'convincing', ',', 'in', 'it', \"'\", 's', 'own', 'way', '.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'this', 'one', ',', 'however', ',', 'looks', 'like', 'a', 'low', 'budget', 'wednesday', 'night', 'special', '.', 'total', 'crap', ',', 'from', 'start', 'to', 'finish', '.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'the', 'plot', 'is', 'so', 'weak', ',', 'you', 'won', \"'\", 't', 'believe', 'it', 'until', 'you', 'see', 'the', 'movie', '(', 'which', 'i', 'would', 'not', 'recommend', 'in', 'the', 'first', 'place', ')', '.', 'i', 'can', 'not', 'point', 'out', 'one', 'actor', 'that', 'actually', 'did', 'a', 'good', 'job', 'in', 'this', 'movie', '.', 'but', 'hey', ',', 'with', 'that', 'script', 'i', 'would', \"'\", 've', 'been', 'surprised', 'if', 'anyone', 'could', 'do', 'a', 'good', 'job', 'acting', '.', 'lots', 'of', 'cl', '##iche', 'scenes', '.', 'c', '##gi', 'looks', 'like', 'it', \"'\", 's', 'taken', 'out', 'of', 'the', \"'\", '86', 'version', '.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'ba', '##h', ',', 'no', ',', 'i', \"'\", 'm', 'getting', 'in', 'a', 'bad', 'mood', 'just', 'writing', 'about', 'this', '.', 'do', 'not', 'watch', 'this', 'movie', '.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'life', \"'\", 's', 'to', 'short', 'to', 'waste', 'it', 'on', 'watching', 'crap', '##py', 'movies', '.']\n"]}],"source":["tokens = tokenizer.convert_ids_to_tokens(vars(train_data.examples[6])['text'])\n","\n","print(tokens)"]},{"cell_type":"markdown","metadata":{"id":"rXJRFP3ip_tW"},"source":["尽管我们已经处理了文本的词汇表，当然也需要为标签构建词汇表。"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7gjVDgJup_tW"},"outputs":[],"source":["LABEL.build_vocab(train_data)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1675607963860,"user":{"displayName":"单元康","userId":"15355905598088806336"},"user_tz":-480},"id":"hEUJb108p_tX","outputId":"00325584-ef4b-4c4a-891a-a0aedcc1a2d2"},"outputs":[{"output_type":"stream","name":"stdout","text":["defaultdict(None, {'neg': 0, 'pos': 1})\n"]}],"source":["print(LABEL.vocab.stoi)"]},{"cell_type":"markdown","metadata":{"id":"TFvykQ6-p_tX"},"source":["像之前一样，我们创建迭代器。根据以往经验，使用最大的batch size可以使transformer获得最好的效果，当然，你也可以尝试一下使用其他的batch size，如果你的显卡比较好的话。"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jqpq1VOZp_tX"},"outputs":[],"source":["BATCH_SIZE = 128\n","\n","\n","train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n","    (train_data, valid_data, test_data), \n","    batch_size = BATCH_SIZE, \n","    device=device)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":340,"status":"ok","timestamp":1675328003676,"user":{"displayName":"单元康","userId":"15355905598088806336"},"user_tz":-480},"id":"2aR8-k-9T2OK","outputId":"289b6fb1-5ba8-4aea-d207-f592fb051c58"},"outputs":[{"name":"stdout","output_type":"stream","text":["Thu Feb  2 08:53:23 2023       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   63C    P0    27W /  70W |   1522MiB / 15360MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|    0   N/A  N/A      3151      C                                    1519MiB |\n","+-----------------------------------------------------------------------------+\n"]}],"source":["!nvidia-smi"]},{"cell_type":"markdown","metadata":{"id":"AyT__DCnp_tX"},"source":["# 6.2 构建模型\n","\n","接下来，我们导入预训练模型。"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":342},"executionInfo":{"elapsed":6,"status":"error","timestamp":1676995938639,"user":{"displayName":"单元康","userId":"15355905598088806336"},"user_tz":0},"id":"JR2d46XVp_tY","outputId":"d89c1f14-bada-4de0-d91c-e1949892ccdf"},"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-1d8d45e67d3f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBertModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mbert\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bert-base-uncased'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}],"source":["from transformers import BertTokenizer, BertModel\n","\n","bert = BertModel.from_pretrained('bert-base-uncased')"]},{"cell_type":"markdown","metadata":{"id":"hpyxSc8hp_tY"},"source":["接下来，我们将定义我们的实际模型。\n","\n","我们将使用预训练的 Transformer 模型，而不是使用embedding层来获取文本的embedding。然后将这些embedding输入GRU以生成对输入句子情绪的预测。我们通过其 config 属性从transformer中获取嵌入维度大小（称为`hidden_size`）。其余的初始化是标准的。\n","\n","在前向传递中，我们将transformer包装在一个`no_grad`中，以确保不会在模型的这部分计算梯度。transformer实际上返回整个序列的embedding以及 *pooled* 输出。 [Bert模型文档](https://huggingface.co/transformers/model_doc/bert.html#transformers.BertModel) 指出，汇集的输出“通常不是输入语义内容的一个很好的总结，你通常更好对整个输入序列的隐藏状态序列进行平均或合并”，因此我们不会使用它。前向传递的其余部分是循环模型的标准实现，我们在最后的时间步长中获取隐藏状态，并将其传递给一个线性层以获得我们的预测。"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HNBmPwiSp_tY"},"outputs":[],"source":["import torch.nn as nn\n","\n","class BERTGRUSentiment(nn.Module):\n","    def __init__(self,\n","                 bert,\n","                 hidden_dim,\n","                 output_dim,\n","                 n_layers,\n","                 bidirectional,\n","                 dropout):\n","        \n","        super().__init__()\n","        \n","        self.bert = bert\n","        \n","        embedding_dim = bert.config.to_dict()['hidden_size']\n","        \n","        self.rnn = nn.GRU(embedding_dim,\n","                          hidden_dim,\n","                          num_layers = n_layers,\n","                          bidirectional = bidirectional,\n","                          batch_first = True,\n","                          dropout = 0 if n_layers < 2 else dropout)\n","        \n","        self.out = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n","        \n","        self.dropout = nn.Dropout(dropout)\n","        \n","    def forward(self, text):\n","        \n","        #text = [batch size, sent len]\n","                \n","        with torch.no_grad():\n","            embedded = self.bert(text)[0]\n","                \n","        #embedded = [batch size, sent len, emb dim]\n","        \n","        _, hidden = self.rnn(embedded)\n","        \n","        #hidden = [n layers * n directions, batch size, emb dim]\n","        \n","        if self.rnn.bidirectional:\n","            hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n","        else:\n","            hidden = self.dropout(hidden[-1,:,:])\n","                \n","        #hidden = [batch size, hid dim]\n","        \n","        output = self.out(hidden)\n","        \n","        #output = [batch size, out dim]\n","        \n","        return output"]},{"cell_type":"markdown","metadata":{"id":"TxwlE8EGp_tY"},"source":["我们使用标准超参数创建模型的实例。"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kragnfIIp_tY"},"outputs":[],"source":["HIDDEN_DIM = 256\n","OUTPUT_DIM = 1\n","N_LAYERS = 2\n","BIDIRECTIONAL = True\n","DROPOUT = 0.25\n","\n","model = BERTGRUSentiment(bert,\n","                         HIDDEN_DIM,\n","                         OUTPUT_DIM,\n","                         N_LAYERS,\n","                         BIDIRECTIONAL,\n","                         DROPOUT)"]},{"cell_type":"markdown","metadata":{"id":"zmVlEbtLp_tZ"},"source":["我们可以检查模型有多少参数，我们的标准型号有不到5M的参数，但这个模型有112M 幸运的是，而且这些参数中有 110M 来自transformer，我们不必再训练它们。"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":778,"status":"ok","timestamp":1675607971221,"user":{"displayName":"单元康","userId":"15355905598088806336"},"user_tz":-480},"id":"Sf2r50ZSp_tZ","outputId":"793251f2-2e29-455e-e120-56296af9efd0"},"outputs":[{"output_type":"stream","name":"stdout","text":["The model has 112,241,409 trainable parameters\n"]}],"source":["def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","print(f'The model has {count_parameters(model):,} trainable parameters')"]},{"cell_type":"markdown","metadata":{"id":"92LHWLxcp_tZ"},"source":["为了固定参数（不需要训练它们），我们需要将它们的 `requires_grad` 属性设置为 `False`。 为此，我们只需遍历模型中的所有 `named_parameters`，如果它们是 `bert` 转换器模型的一部分，我们设置 `requires_grad = False`，如微调的话，需要将`requires_grad`设置为`True`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s3dBXO7Ep_tZ"},"outputs":[],"source":["for name, param in model.named_parameters():                \n","    if name.startswith('bert'):\n","        param.requires_grad = False"]},{"cell_type":"markdown","metadata":{"id":"eG0jUMxmp_ta"},"source":["我们现在可以看到我们的模型有不到3M的可训练参数，这使得它几乎可以与`FastText`模型相媲美。 然而，文本仍然必须通过transformer传播，这导致训练需要更长的时间。"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1675607972206,"user":{"displayName":"单元康","userId":"15355905598088806336"},"user_tz":-480},"id":"HRJkTg3Wp_ta","outputId":"ff36752c-f486-4523-9b48-fe4bf8fe76c5"},"outputs":[{"output_type":"stream","name":"stdout","text":["The model has 2,759,169 trainable parameters\n"]}],"source":["def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","print(f'The model has {count_parameters(model):,} trainable parameters')"]},{"cell_type":"markdown","metadata":{"id":"5ind6u9Sp_ta"},"source":["我们可以仔细检查可训练参数的名称，确保它们有意义。 我们可以看到，它们都是 GRU（`rnn`）和线性层（`out`）的参数。"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1675607974010,"user":{"displayName":"单元康","userId":"15355905598088806336"},"user_tz":-480},"id":"NKxcjjukp_ta","outputId":"f7b34698-ade2-444f-ccc0-030147baa20d","scrolled":true},"outputs":[{"output_type":"stream","name":"stdout","text":["rnn.weight_ih_l0\n","rnn.weight_hh_l0\n","rnn.bias_ih_l0\n","rnn.bias_hh_l0\n","rnn.weight_ih_l0_reverse\n","rnn.weight_hh_l0_reverse\n","rnn.bias_ih_l0_reverse\n","rnn.bias_hh_l0_reverse\n","rnn.weight_ih_l1\n","rnn.weight_hh_l1\n","rnn.bias_ih_l1\n","rnn.bias_hh_l1\n","rnn.weight_ih_l1_reverse\n","rnn.weight_hh_l1_reverse\n","rnn.bias_ih_l1_reverse\n","rnn.bias_hh_l1_reverse\n","out.weight\n","out.bias\n"]}],"source":["for name, param in model.named_parameters():                \n","    if param.requires_grad:\n","        print(name)"]},{"cell_type":"markdown","metadata":{"id":"VA88j8EMp_ta"},"source":["## 6.3 训练模型\n","\n","按照惯例，我们构建自己的模型评价标准(损失函数)，仍然是二分类"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xW-KBOz0p_ta"},"outputs":[],"source":["import torch.optim as optim\n","\n","optimizer = optim.Adam(model.parameters())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vcFWw1LLp_ta"},"outputs":[],"source":["criterion = nn.BCEWithLogitsLoss()"]},{"cell_type":"markdown","metadata":{"id":"tSo2upcsp_tb"},"source":["将模型和评价标准（损失函数）放在 GPU 上，如果你有GPU的话"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1675607977671,"user":{"displayName":"单元康","userId":"15355905598088806336"},"user_tz":-480},"id":"gRxYN77JV88C","outputId":"59d94964-0781-4f7e-d5cc-d80d696b20e9"},"outputs":[{"output_type":"stream","name":"stdout","text":["True\n"]}],"source":["import torch\n","print(torch.cuda.is_available())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EL9EIEMop_tb"},"outputs":[],"source":["torch.cuda.empty_cache()\n","model = model.to(device)\n","criterion = criterion.to(device)"]},{"cell_type":"raw","metadata":{"id":"yJJbu8Gfp_tb"},"source":["接下来，我们将定义函数用于：计算准确度、定义train、evalute函数以及计算训练/评估时期每一个epoch所需的时间。"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ELMbfVQvp_tb"},"outputs":[],"source":["def binary_accuracy(preds, y):\n","    \"\"\"\n","    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n","    \"\"\"\n","\n","    #round predictions to the closest integer\n","    rounded_preds = torch.round(torch.sigmoid(preds))\n","    correct = (rounded_preds == y).float() #convert into float for division \n","    acc = correct.sum() / len(correct)\n","    return acc"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jxn3qBGap_tb"},"outputs":[],"source":["def train(model, iterator, optimizer, criterion):\n","    \n","    epoch_loss = 0\n","    epoch_acc = 0\n","    \n","    model.train()\n","    \n","    for idx, batch in enumerate(iterator):\n","        \n","        optimizer.zero_grad()\n","        \n","        predictions = model(batch.text).squeeze(1)\n","        \n","        loss = criterion(predictions, batch.label)\n","        \n","        acc = binary_accuracy(predictions, batch.label)\n","        \n","        loss.backward()\n","        \n","        optimizer.step()\n","        \n","        epoch_loss += loss.item()\n","        epoch_acc += acc.item()\n","\n","        if (idx+1) % 250 == 0:\n","          print(\"====\", idx, \"acc:\", epoch_acc/idx, \"loss:\", epoch_loss/idx, \"====\")\n","        \n","    return epoch_loss / len(iterator), epoch_acc / len(iterator)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bSk2Cim5p_tb"},"outputs":[],"source":["def evaluate(model, iterator, criterion):\n","    \n","    epoch_loss = 0\n","    epoch_acc = 0\n","    \n","    model.eval()\n","    \n","    with torch.no_grad():\n","    \n","        for batch in iterator:\n","\n","            predictions = model(batch.text).squeeze(1)\n","            \n","            loss = criterion(predictions, batch.label)\n","            \n","            acc = binary_accuracy(predictions, batch.label)\n","\n","            epoch_loss += loss.item()\n","            epoch_acc += acc.item()\n","        \n","    return epoch_loss / len(iterator), epoch_acc / len(iterator)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-G6kBeymp_tb"},"outputs":[],"source":["import time\n","\n","def epoch_time(start_time, end_time):\n","    elapsed_time = end_time - start_time\n","    elapsed_mins = int(elapsed_time / 60)\n","    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n","    return elapsed_mins, elapsed_secs"]},{"cell_type":"markdown","metadata":{"id":"mrPH07VBp_tc"},"source":["最后，我们将训练我们的模型。 由于transformer的尺寸的原因，这比以前的任何型号都要长得多。 即使我们没有训练任何transformer的参数，我们仍然需要通过模型传递数据，这在标准 GPU 上需要花费大量时间。"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7jrrhjbIp_tc","colab":{"base_uri":"https://localhost:8080/","height":238},"executionInfo":{"status":"error","timestamp":1676995912837,"user_tz":0,"elapsed":218,"user":{"displayName":"单元康","userId":"15355905598088806336"}},"outputId":"2cf1369c-657b-4aed-e6a9-b901fa8595d1"},"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-9953c53b9f3c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN_EPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'time' is not defined"]}],"source":["import matplotlib.pyplot as plt\n","N_EPOCHS = 3\n","\n","best_valid_loss = float('inf')\n","\n","for epoch in range(N_EPOCHS):\n","    \n","    start_time = time.time()\n","    \n","    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n","    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n","        \n","    end_time = time.time()\n","        \n","    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n","        \n","    if valid_loss < best_valid_loss:\n","        best_valid_loss = valid_loss\n","        torch.save(model.state_dict(), '/content/drive/MyDrive/tut6-model.pt')\n","    \n","    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n","    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n","    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"]},{"cell_type":"markdown","metadata":{"id":"FU5vCDpMp_tc"},"source":["我们将加载为我们提供最佳验证集上损失值的参数，并在测试集上应用这些参数 - 并在测试集上达到了最优的结果。"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uIyV2K2mp_tc","executionInfo":{"status":"ok","timestamp":1675611170909,"user_tz":-480,"elapsed":465708,"user":{"displayName":"单元康","userId":"15355905598088806336"}},"outputId":"a3361ace-2723-40ae-94e3-d51162594589"},"outputs":[{"output_type":"stream","name":"stdout","text":["Test Loss: 0.249 | Test Acc: 89.65%\n"]}],"source":["model.load_state_dict(torch.load('./tut6-model.pt'))\n","\n","test_loss, test_acc = evaluate(model, test_iterator, criterion)\n","\n","print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"]},{"cell_type":"markdown","metadata":{"id":"1_5gPU1bp_tc"},"source":["## 6.4 模型验证\n","\n","然后我们将使用该模型来测试一些序列的情绪。 我们对输入序列进行标记，将其修剪到最大长度，将特殊token添加到任一侧，将其转换为张量，使用unsqueeze函数增加一维，然后将其传递给我们的模型。"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BMXacqvkp_tc"},"outputs":[],"source":["def predict_sentiment(model, tokenizer, sentence):\n","    model.eval()\n","    tokens = tokenizer.tokenize(sentence)\n","    tokens = tokens[:max_input_length-2]\n","    indexed = [init_token_idx] + tokenizer.convert_tokens_to_ids(tokens) + [eos_token_idx]\n","    tensor = torch.LongTensor(indexed).to(device)\n","    tensor = tensor.unsqueeze(0)\n","    prediction = torch.sigmoid(model(tensor))\n","    return prediction.item()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ViYTZyksp_tc","executionInfo":{"status":"ok","timestamp":1675613596777,"user_tz":-480,"elapsed":3,"user":{"displayName":"单元康","userId":"15355905598088806336"}},"outputId":"4f43bff2-2840-4f8c-ef62-b9b2c81aef32"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.02770855277776718"]},"metadata":{},"execution_count":47}],"source":["predict_sentiment(model, tokenizer, \"This film is terrible\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OU7k01QPp_td","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1675613600237,"user_tz":-480,"elapsed":1143,"user":{"displayName":"单元康","userId":"15355905598088806336"}},"outputId":"1b527f9b-c88b-47e1-8d05-b8333e4be047"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.9556320905685425"]},"metadata":{},"execution_count":48}],"source":["predict_sentiment(model, tokenizer, \"This film is great\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SWIs17njVZj_"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.10"}},"nbformat":4,"nbformat_minor":0}